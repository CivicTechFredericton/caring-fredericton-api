Link -------> https://docs.google.com/document/d/1d5zvdFNZDDyWCI9i2Qpj8ehimTzP0K4C15zm9bwRQgk/edit?usp=sharing (Easier to read)






In this doc, I want to go over, step by step, how one might deploy a serverless API using the serverless starter kit and integrate that with other AWS services. We will be going over the following topics throughout the course of the reading:

Create an AWS account and download credentials
Assume a role using the aws cli
Log in using IAM credentials
Create a serverless API function in the starter kit
Use dynamodb to perform actions with that API
Deploy the API locally for testing
Deploy the API to the CivicTech account
Tear down the services that were just created.

I will try to make this as simple as possible as I know there are varying levels of experience with AWS, I myself am no expert. Each section will be designed to be standalone so feel free to skip the ones you have experience in. Further reading will be provided at the end of each section (AWS has great documentation). Let’s get started.
Create an AWS Account and Download Credentials
***
Note: For the Caring Calendar, IAM roles will be distributed so creating your own account will not be necessary but some of the steps in this section will still be necessary to use the aws command line interface
***
1.1 Create an AWS account.
In this section we will go over the process of creating an AWS account if you do not already have one. Creating a new account gives you access to the “Free Tier” of AWS services which allows you to play around without incurring much of a charge for a full year.
***Warning***
NOT ALL AWS SERVICES ARE FREE UNDER FREE TIER! Services like AWS Kinesis and Sagemaker will incur a large charge if left alone even if nothing is being passed through those services. That being said, all services used in this tutorial (And hopefully in the caring calendar as a whole), will be covered under the free tier up to certain limits.

*****

Now that I have made that clear, let’s create an account so we can get started with serverless!

Step 1- Navigate to the console:
Navigate to aws.amazon.com and click on the big yellow “Sign into the console” button in the top right hand corner of the screen.



Step 2- Create an Account:
Click the “Create a new AWS account” button at the bottom of the login form and you will be redirected to a page where you can enter all your credentials. AWS will ask you to enter a code received when they call you to verify the account.

Once the process is complete, navigate back to the console and sign into you new account using root credentials. You should be presented with a page that looks something along the lines of this:


Congratulations! You’re in!

1.2- Downloading account credentials
Now that you have an account, you’ll need to be able to access that account from the aws command line interface which we will install in the next section. In order to do that, you will need to save the AWS Access Key and Secret Access Key from the Identity Access Manager (IAM) found in the AWS console.

IAM is a service that allows you to manage everything related to security on your account. From who has access to which services and which services have access to which services, to enabling multi factor authentication on you account, IAM has you covered.

Step 1- Navigate to IAM
In the console, at the top of the page, there is a search bar. Start typing “IAM” into it and you should be presented with a dropdown of services matching your query. Select IAM from the dropdown and you will be presented with this page:




If you have just set up an AWS account, none of those check marks will be there so it is a good idea to go through and complete all five tasks IAM has laid out for you to make sure the account is secure.

Step 2- Download Credentials:
Once you have all 5 check marks lit up, it is time to download your credentials. Navigate over to the “Users” tab at the left hand side of the screen and then click “Add User” at the top of that table. For the username, enter something along the lines of Administrator and give yourself both Programmatic access and Console access. The programmatic access will be used to log in through the cli in a later section. Create the password as desired, if autogenerated is chosen, it will be emailed to you.

Click on the Next: Permissions button at the bottom of the page and you will be greeted with a “groups” page that looks like this:



Depending on where you are in your AWS career, you may or may not have groups already created on this page. For the sake of the tutorial, we will create a new group giving administrator access to our administrators. Follow the steps below to do that:

Click Create Group
For the group name, enter sys-admin or anything you feel like.
In the Policy table, search for the AdministratorAccess policy and click the check mark beside the name.
Click Create Group

Now you have an administrator group! Select it and then click on Next: Review followed by Create User on the next page if you are happy with the configuration.

You should now be presented with a page describing your user with the secret access key and the access key. These are they keys we have been looking for! Click on the Download .csv button to download a .csv file with your credentials. Make sure to keep them in a safe file on your computer.





*****Note****
AWS Access Keys and Secret Keys are not permanent. They can be randomly generated at any time from the Users tab in the IAM console. The Secret Key is only ever shown to you one time at the creation of your user. If you lose this key, you cannot get it back and will have to regenerate both keys again.
**************


2- Assume a Role Using the AWS CLI
Now you have an account created and would like to start adding to the Civic Tech AWS resources. Only problem is, you don’t yet have access to the civic tech account. In order to do this, you are going to have to “Assume a Role” inside the Civic Tech account. Essentially this just refers to gaining access to the account through a Role inside the Civic Tech account. Remember IAM? This is all done through that service as well!
2.1- Downloading the AWS CLI
The first step in assuming a role, is to get the aws cli installed on your machine. While I could go through all the process here, I figure it’s best that AWS handle the explanation for different operating systems. Follow the steps in the following link to get awscli installed on your machine:

https://docs.aws.amazon.com/cli/latest/userguide/awscli-install-bundle.html

To test the installation, run:
aws --version

And you should be presented with something along the lines of:

aws-cli/1.11.13 Python/3.5.2 Linux/4.15.0-24-generic botocore/1.4.70

Now that you have the cli installed, we can set up your civic tech profile.

2.2- Setting up AWS Profiles
AWS allows you to access multiple accounts through the same root account by using something called Profiles this will let you access the Civic Tech account through your personal account you set up in the first section. In this section we will set up your default profile (personal account) and use that to set up a Civic Tech profile.
Step 1- Setting up a Default Profile:
To set up a default profile, we will be using the access key and secret key you obtained in the first section so make sure you have those accessible. Now, open up a terminal and follow along with me:

1. Run the following command to start adding your default profile:
aws configure

2. AWS will run through a series of fields you should fill out including your access key, secret key, default region (use us-east-1 if unsure) and default output which should be set to json. Fill those out then move onto the next step.

Great! You now have a default profile set up. Let’s now set one up for civic tech!

Step 2- Setting up Additional Profiles
Now that we have the aws cli working for our default profile, we can now set up a civictech profile so our serverless application deploys our resources to the civic tech AWS account and not to our personal account. Open up another terminal and your favourite text editor and follow along below:

1. In your terminal, run the following command to enter the aws folder:
cd ~/.aws/

2. In your text editor, open up the file called credentials.
vim credentials

3. Once open, add the following text to the file (I’ll go over what it does in a second):
[default]
aws_access_key_id = **Your Access Key for default Account**
aws_secret_access_key = **Your Secret Key for default Account

[civictech]
region=ca-central-1
output=json
source_profile=default
role_arn = arn:aws:iam::260286112360:role/**Role**

4. Save and close the file.

Above, we created a new profile called civictech as well as added the default profile if it was not already there. As you can see, the civictech profile references the default profile in the source_profile field. In order to access the Civic Tech AWS account, someone has to grant you access. They do that through your default profile so the role needs to reference that profile when being set up.

The next field of interest is the role_arn field. Which specifies the specific role you will be assuming in the Civic Tech account. This allows the owner of the account to grant specific permissions (such as serverless resources) for your access.

That’s it! We now have everything set up to be able to deploy a serverless application, so let’s go ahead and start creating one to deploy below.

Further reading (AWS version of what I wrote here):
https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html
3. Signing in using IAM credentials
If an account has already been created for you in an AWS account, it is possible to sign in directly through the console without having to create your own account. Once you have signed into the AWS console, you will need to set up a user and download your security keys as laid out in section 1. Once that is complete, you will need to assume the role in the aws cli as done in section 2.

4. Create a Serverless API in the Starter Kit
In this section we will create a new service that we will be able to hit with our API locally. In the next sections, we will deploy it to the aws account to see it in action under a real url.

Step 1- Creating a New Service
For this example I will be adding an events service to the caring calendar project so I can kill two birds with one stone. This service will allow users to receive events, as well as post new events to be added to the calendar. Extensions of this service would be to edit an event and to delete an event. Follow the steps below to add a service to the backend.

1. If you have not already cloned the repository locally, clone it by running the following command from the desired parent directory in the terminal:
git clone https://github.com/CivicTechFredericton/caring-fredericton-api.git

2. Navigate into the repository by running:
cd caring-fredericton-api

3. If this is your first time inside the repository, run the command:
npm install

To install all javascript dependencies required for the starter kit (We will get into that more later).
4. Open the repository in your favourite text editor (I use atom) and navigate to the services folder. Open it up and you should see 3 (or 4) folders called: error, root, todo, and (maybe) dogs. These are all the services currently configured in the starter kit. They contain code necessary to run specific tasks using AWS API Gateway and AWS Lambda.

5. We will create a new folder and call it events. Once the folder is created, we will add three files to the folder called model.py, resource.py, and routes.py. These files will contain a definition of the Event item, a definition of the schema, and the definition of the API functions respectively.

Add the following code to the model.py file:

from pynamodb.models import Model
from pynamodb.attributes import UnicodeAttribute, BooleanAttribute, UTCDateTimeAttribute, NumberAttribute


class EventModel(Model):
    class Meta:
            simple_name = 'event'

    id = UnicodeAttribute(hash_key=True)
    title = UnicodeAttribute()
    description = UnicodeAttribute(null=True)
    start_date = UTCDateTimeAttribute(null=False)
    end_date = UTCDateTimeAttribute(null=False)
    location = UnicodeAttribute(null=False)
    capacity = NumberAttribute(null= True)

This describes the way our event will look (for now that is, we may want to change it at a later date). As you can see, the attributes added to the event model are typed using the pynamodb.attributes module. The complete module can be found at the following link:

https://github.com/pynamodb/PynamoDB/blob/master/pynamodb/attributes.py

Notice the hash_key attribute added to the id field. Every DynamoDB table has a hash key and so you must specify which attribute is the hash key for each Model you define (http://pynamodb.readthedocs.io/en/latest/tutorial.html).

In the Meta class, you can also define which region the dynamo table resides in, it’s read capacity, write capacity, and many other things.


Now that we have our model defined, we can add some code to the resources.py file which will define our schema for the Event service.

from core.resource import ma
from marshmallow import fields


class EventSchema(ma.Schema):
    class Meta:
            strict = True

    id = fields.String(required=True)
    title = fields.String(required=True)
    description = fields.String(required=False)
    start_date = fields.String(required=True)
    end_date = fields.String(required=True)
    location = fields.String(required=True)
    capacity = fields.String(required=False)


event_schema = EventSchema()



This class defines which attributes the user must provide to the API and which attributes can be left out. Although this is done to an extent in the model.py file, the null=True/False statement is not required by pynamodb and therefore can easily be left out without any errors. The schema file ensures the models are structured in a strict fashion.

Now for the fun part, let’s add some code to the routes.py file to be able to GET events and POST new events to the events table:

from flask import Blueprint, jsonify
from .model import EventModel
from .resource import event_schema
from webargs.flaskparser import use_kwargs
from datetime import datetime

import logging
logger = logging.getLogger(__name__)
blueprint = Blueprint('event', __name__)


@blueprint.route('/events', methods=["GET"])
def list_event_items():
    event_items = EventModel.scan()

    response = []

    for item in event_items:
        response.append({
            'id': item.id,
            'title': item.title,
            'description': item.description,
            'start_date': datetime.strptime(item.start_date, '%Y-%m-%dT%H:%M:%SZ'),
            'end_date': item.end_date,
            'location': item.location,
            'capacity': item.capacity
        })

    return jsonify(response)


@blueprint.route('/events', methods=["POST"])
@use_kwargs(event_schema, locations=('json',))
def add_event_item(**kwargs):
    event_item = EventModel(id=kwargs['id'],
                          title=kwargs['title'],
                          description=kwargs['description'],
                          start_date=datetime.strptime(kwargs['start_date'], '%Y-%m-%dT%H:%M:%SZ'),
                          end_date = datetime.strptime(kwargs['end_date'], '%Y-%m-%dT%H:%M:%SZ'),
                          capacity = kwargs['capacity'],
                          location = kwargs['location'])

    event_item.save()

    response = jsonify(event_schema.dump(event_item).data)
    response.status_code = 201

    return response



In order for this block of code to work, you will need a dynamo table to send the events to. Modify the cloudformation/dev/dynamo_tables.py to add the event table:

DynamoDBRole:
    DeletionPolicy: Delete
    Properties:
      TableName: caring-fred-dev-event
      AttributeDefinitions:
      - AttributeName: id
        AttributeType: S
      KeySchema:
      - AttributeName: id
        KeyType: HASH
      ProvisionedThroughput:
        ReadCapacityUnits: 1
        WriteCapacityUnits: 1
Type: AWS::DynamoDB::Table



To make sure this table is actually created, you will need to run the cloudformation/dynamo_tables.py script with the right environment and profile defined.

5. Deploying

Set the AWS_PROFILE environment variable (for example: AWS_PROFILE=caringcalendar)

Deploying the serverless application is relatively easy. Just run
npm run deploy -- --stage dev

To deploy the services to the caring calendar account, or:

npm run local -- --stage dev

To run the changes locally! Of course, you would change the environment to match what you wanted to do. It is best practice to deploy to an environment with your name attached to it for test purposes. For example: andrew-test

6. Testing
Authentication is needed to test the API and all the authentication is done through the cognito API. When the frontend is set up to deal with the registration requests, you can obtain a jwt token once you have signed in and use that in the ‘Authorization’ header on the API.

Check this out for some more information:
https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-user-pools-authentication-flow.html#amazon-cognito-user-pools-server-side-authentication-flow


When you deploy, you will be given a url that represents your API endpoint. Type that into your favourite service (like postman) and you should be able to hit the API and add events to an event table!

7. Tear down
If a mistake was made, or the services are no longer needed, the command:

npm run remove -- --stage dev

Can be run to remove the services in your current working directory.

https://serverless.com/framework/docs/providers/aws/cli-reference/remove/
